{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "normal-alcohol",
   "metadata": {},
   "source": [
    "# Lab 1 - First steps with Spark\n",
    "\n",
    "At this end of this lab :\n",
    "\n",
    "- export your notbook : File > Export Notebook As ... > Export Notebook To HTML\n",
    "- An turn your cluster off !\n",
    "    \n",
    "## Creating a Spark session\n",
    "\n",
    "Write in a new cell :\n",
    "\n",
    "- `spark`  to chech if everithing is ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4e066e91-b825-4fd0-9f58-de443b8e7b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession \n",
    "         .builder\n",
    "         .appName(\"Lab 1\")\n",
    "         .master(\"local[5]\") #parall√©lisation sur 5 threads\n",
    "         .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "classical-willow",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-583919-0.jupyter-583919.user-nrandriamanana.svc.cluster.local:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>k8s://https://kubernetes.default.svc:443</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4cceab99d0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:=========================================================(5 + -4) / 4]\r"
     ]
    }
   ],
   "source": [
    "#Spark session\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-signal",
   "metadata": {},
   "source": [
    "## üíæFirst steps with Spark - Data importation\n",
    "\n",
    "Spark's main object class is the DataFrame, which is a distributed table. It is analogous to R's or Python (Pandas)'s data frames: one row represents an observation, one column represents a variable. But contrary to R or Python, Spark's DataFrames can be distributed over hundred of nodes.\n",
    "\n",
    "Spark support multiple data formats, and multiple  ways to load them.\n",
    "\n",
    "- data format : csv, json, parquet (an open source column oriented format)\n",
    "- can read archive files\n",
    "- schema detection or user defined schema. For static data, like a json file, schema detection can be use with good results.\n",
    "\n",
    "Spark has multiple syntaxes to import data. Some are simple with no customisation, others are more complexes but you can specify options.\n",
    "\n",
    "The simplest syntaxes to load a json or a csv file are :\n",
    "\n",
    "```python\n",
    "# JSON\n",
    "json_df = spark.read.json([location of the file])\n",
    "# csv\n",
    "csv_df = spark.read.csv([location of the file])\n",
    "\n",
    "```\n",
    "\n",
    "In the future, you may consult the [Data Source documentation](https://spark.apache.org/docs/latest/sql-data-sources.html) to have the complete description of Spark's reading abilities.\n",
    "\n",
    "The data you will use in this lab are real data from the twitter [sampled stream API](https://developer.twitter.com/en/docs/twitter-api/tweets/sampled-stream/introduction) and [filtered stream API](https://developer.twitter.com/en/docs/twitter-api/tweets/filtered-stream/introduction). The tweets folder contains more than 50 files and more than 2 million tweets. The tweets was collected between the 14/04/2021 and the 18/04/2021. The total collection time was less than 10 hours.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úçHands-on 1  - Data importation\n",
    "\n",
    "- Load the json file store here : `s3a://nrandriamanana/diffusion/formation/data/tweets/tweets20220414-090219.jsonl.gz` and name you data frame `df_tweet`\n",
    "\n",
    "  <small> ‚öôÔ∏è This file is an a `JSONL` (JSON-line) format, which means that each line of it is a JSON object. A JSON object is just a Python dictionary or a JavaScript object and looks like this: `{ key1: value1, key2: [\"array\", \"of\", \"many values]}`). This file has been compressed into a `GZ` archive, hence the `.jsonl.gz` ending. Also this file is not magically appearing in your S3 storage. It is hosted on one of your teacher's bucket and has been made public, so that you can access it.</small>\n",
    "\n",
    "- It's possible to load multiple file in a unique DataFrame. It's useful when you have daily files and want to process them all. It's the same syntax as the previous one, just specify a folder. Like `s3a://nrandriamanana/diffusion/formation/data/tweets/`. Name you DataFrame `df_tweet_big`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "preliminary-criminal",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[data: struct<author_id:string,created_at:string,entities:struct<annotations:array<struct<end:bigint,normalized_text:string,probability:double,start:bigint,type:string>>,cashtags:array<struct<end:bigint,start:bigint,tag:string>>,hashtags:array<struct<end:bigint,start:bigint,tag:string>>,mentions:array<struct<end:bigint,id:string,start:bigint,username:string>>,urls:array<struct<description:string,display_url:string,end:bigint,expanded_url:string,images:array<struct<height:bigint,url:string,width:bigint>>,start:bigint,status:bigint,title:string,unwound_url:string,url:string>>>,id:string,lang:string,possibly_sensitive:boolean,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,source:string,text:string,withheld:struct<copyright:boolean,country_codes:array<string>>>, includes: struct<users:array<struct<created_at:string,id:string,name:string,username:string,verified:boolean,withheld:struct<country_codes:array<string>>>>>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame creation\n",
    "df_tweet = spark.read.json(\"s3a://nrandriamanana/diffusion/formation/data/tweets/tweets20220414-090219.jsonl.gz\")\n",
    "df_tweet_big = spark.read.json(\"s3a://nrandriamanana/diffusion/formation/data/tweets/\")\n",
    "\n",
    "# caching\n",
    "df_tweet.cache()\n",
    "df_tweet_big.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-advertising",
   "metadata": {},
   "source": [
    "Printing schema and some rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "unnecessary-quarter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                data|            includes|\n",
      "+--------------------+--------------------+\n",
      "|{1037039682822070...|{[{2018-09-04T18:...|\n",
      "|{1391192446613160...|{[{2021-05-09T00:...|\n",
      "|{3498010513, 2022...|{[{2015-09-08T23:...|\n",
      "|{1203504398321647...|{[{2019-12-08T02:...|\n",
      "|{3111644864, 2022...|{[{2015-03-27T20:...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- author_id: string (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- entities: struct (nullable = true)\n",
      " |    |    |-- annotations: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- end: long (nullable = true)\n",
      " |    |    |    |    |-- normalized_text: string (nullable = true)\n",
      " |    |    |    |    |-- probability: double (nullable = true)\n",
      " |    |    |    |    |-- start: long (nullable = true)\n",
      " |    |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- cashtags: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- end: long (nullable = true)\n",
      " |    |    |    |    |-- start: long (nullable = true)\n",
      " |    |    |    |    |-- tag: string (nullable = true)\n",
      " |    |    |-- hashtags: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- end: long (nullable = true)\n",
      " |    |    |    |    |-- start: long (nullable = true)\n",
      " |    |    |    |    |-- tag: string (nullable = true)\n",
      " |    |    |-- mentions: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- end: long (nullable = true)\n",
      " |    |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |    |-- start: long (nullable = true)\n",
      " |    |    |    |    |-- username: string (nullable = true)\n",
      " |    |    |-- urls: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |    |-- end: long (nullable = true)\n",
      " |    |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |    |-- images: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |    |-- start: long (nullable = true)\n",
      " |    |    |    |    |-- status: long (nullable = true)\n",
      " |    |    |    |    |-- title: string (nullable = true)\n",
      " |    |    |    |    |-- unwound_url: string (nullable = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- possibly_sensitive: boolean (nullable = true)\n",
      " |    |-- public_metrics: struct (nullable = true)\n",
      " |    |    |-- like_count: long (nullable = true)\n",
      " |    |    |-- quote_count: long (nullable = true)\n",
      " |    |    |-- reply_count: long (nullable = true)\n",
      " |    |    |-- retweet_count: long (nullable = true)\n",
      " |    |-- source: string (nullable = true)\n",
      " |    |-- text: string (nullable = true)\n",
      " |    |-- withheld: struct (nullable = true)\n",
      " |    |    |-- copyright: boolean (nullable = true)\n",
      " |    |    |-- country_codes: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |-- includes: struct (nullable = true)\n",
      " |    |-- users: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- created_at: string (nullable = true)\n",
      " |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- username: string (nullable = true)\n",
      " |    |    |    |-- verified: boolean (nullable = true)\n",
      " |    |    |    |-- withheld: struct (nullable = true)\n",
      " |    |    |    |    |-- country_codes: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                data|            includes|\n",
      "+--------------------+--------------------+\n",
      "|{8566371157357076...|{[{2017-04-24T22:...|\n",
      "|{1100882149060812...|{[{2019-02-27T22:...|\n",
      "|{1647610112, 2022...|{[{2013-08-05T11:...|\n",
      "|{1227057358683066...|{[{2020-02-11T02:...|\n",
      "|{287239880, 2022-...|{[{2011-04-24T16:...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- author_id: string (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- entities: struct (nullable = true)\n",
      " |    |    |-- annotations: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- end: long (nullable = true)\n",
      " |    |    |    |    |-- normalized_text: string (nullable = true)\n",
      " |    |    |    |    |-- probability: double (nullable = true)\n",
      " |    |    |    |    |-- start: long (nullable = true)\n",
      " |    |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- cashtags: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- end: long (nullable = true)\n",
      " |    |    |    |    |-- start: long (nullable = true)\n",
      " |    |    |    |    |-- tag: string (nullable = true)\n",
      " |    |    |-- hashtags: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- end: long (nullable = true)\n",
      " |    |    |    |    |-- start: long (nullable = true)\n",
      " |    |    |    |    |-- tag: string (nullable = true)\n",
      " |    |    |-- mentions: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- end: long (nullable = true)\n",
      " |    |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |    |-- start: long (nullable = true)\n",
      " |    |    |    |    |-- username: string (nullable = true)\n",
      " |    |    |-- urls: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |    |-- end: long (nullable = true)\n",
      " |    |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |    |-- images: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |    |-- start: long (nullable = true)\n",
      " |    |    |    |    |-- status: long (nullable = true)\n",
      " |    |    |    |    |-- title: string (nullable = true)\n",
      " |    |    |    |    |-- unwound_url: string (nullable = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- possibly_sensitive: boolean (nullable = true)\n",
      " |    |-- public_metrics: struct (nullable = true)\n",
      " |    |    |-- like_count: long (nullable = true)\n",
      " |    |    |-- quote_count: long (nullable = true)\n",
      " |    |    |-- reply_count: long (nullable = true)\n",
      " |    |    |-- retweet_count: long (nullable = true)\n",
      " |    |-- source: string (nullable = true)\n",
      " |    |-- text: string (nullable = true)\n",
      " |    |-- withheld: struct (nullable = true)\n",
      " |    |    |-- copyright: boolean (nullable = true)\n",
      " |    |    |-- country_codes: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |-- includes: struct (nullable = true)\n",
      " |    |-- users: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- created_at: string (nullable = true)\n",
      " |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- username: string (nullable = true)\n",
      " |    |    |    |-- verified: boolean (nullable = true)\n",
      " |    |    |    |-- withheld: struct (nullable = true)\n",
      " |    |    |    |    |-- country_codes: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_tweet.show(5)\n",
    "df_tweet.printSchema()\n",
    "df_tweet_big.show(5)\n",
    "df_tweet_big.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-spencer",
   "metadata": {},
   "source": [
    "## ü•âData frame basic manipulations\n",
    "\n",
    "If DataFrames are immutable, they can however be **_transformed_** in other DataFrames, in the sense that a modified copy is returned. Such **transformations** include: filtering, sampling, dropping columns, selecting columns, adding new columns...\n",
    "\n",
    "First, you can get information about the columns with:\n",
    "\n",
    "```python\n",
    "df.columns       # get the column names\n",
    "df.schema        # get the column names and their respective type\n",
    "df.printSchema() # same, but human-readable\n",
    "```\n",
    "\n",
    "You can select columns with the `select()` method. It takes as argument a list of column name. For example :\n",
    "\n",
    "```python\n",
    "df_with_less_columns = df\\\n",
    "  .select(\"variable3\",\"variable_four\",\"variable-6\")\n",
    "\n",
    "# Yes, you do need the ugly \\ at the end of the line,\n",
    "# if you want to chain methods between lines in Python\n",
    "```\n",
    "\n",
    "You can get nested columns easily with :\n",
    "\n",
    "```python\n",
    "df.select(\"parentField.nestedField\")\n",
    "```\n",
    "\n",
    "To filter data you could use the `filter()` method. It take as input an expression that gets evaluated for each observation and should return a boolean. Sampling is performed with the `sample()` method. For example :\n",
    "\n",
    "```python\n",
    "df_with_less_rows = df\\\n",
    "  .sample(fraction=0.001)\\\n",
    "  .filter(df.variable1==\"value\")\\\n",
    "  .show(10)\n",
    "```\n",
    "\n",
    "As said before your data are distributed over multiple nodes (executors) and data inside a node are split into partitions. Then each transformations will be run in parallel. They are called *narrow transformation* For example, to sample a DataFrame, Spark sample every partitions in parallel because sample all partition produce the sample DataFrame. For some transformations, like `groupBy()` it's impossible, and it's cannot be run in parallel.\n",
    "\n",
    "![](https://raw.githubusercontent.com/HealerMikado/panorama_big_data_2021/main/labs/lab%202%20-%20first%20steps%20with%20Spark/img/spark_exemple1_pipeline.png)\n",
    "\n",
    "<!-- take() collect() limit() first() show() -->\n",
    "<!-- lien vers la doc https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.sql.html#dataframe-apis -->\n",
    "\n",
    "### üò¥Lazy evaluation\n",
    "\n",
    "This is because Spark has what is known as **lazy evaluation**, in the sense that it will wait as much as it can before performing the actual computation. Said otherwise, when you run an instruction such as:\n",
    "\n",
    "```python\n",
    "tweet_author_hashtags = df_tweet_big.select(\"auteur\",\"hashtags\")\n",
    "```\n",
    "\n",
    "... you are not executing anything! Rather, you are building an **execution plan**, to be realised later.\n",
    "\n",
    "Spark is quite extreme in its laziness, since only a handful of methods called **actions**, by opposition to **transformations**, will trigger an execution. The most notable are:\n",
    "\n",
    "1. `collect()`, explicitly asking Spark to fetch the resulting rows instead of to lazily wait for more instructions,\n",
    "2. `take(n)`, asking for `n` first rows\n",
    "3. `first()`, an alias for `take(1)`\n",
    "4. `show()` and `show(n)`, human-friendly alternatives[^5]\n",
    "5. `count()`, asking for the numbers of rows\n",
    "6. all the \"write\" methods (write on file, write to database), see [here](https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.sql.html#input-and-output) for the list\n",
    "\n",
    "[^5]: `first()` is exactly `take(1)` ([ref]( https://stackoverflow.com/questions/37495039/difference-between-spark-rdds-take1-and-first)) and show prints the result instead of returning it as a list of rows ([ref](https://stackoverflow.com/questions/53884994/what-is-the-difference-between-dataframe-show-and-dataframe-take-in-spark-t))\n",
    "\n",
    "**This has advantages:** on huge data, you don't want to accidently perform a computation that is not needed. Also, Spark can optimize each **stage** of the execution in regard to what comes next. For instance, filters will be executed as early as possible, since it diminishes the number of rows on which to perform later operations. On the contrary, joins are very computation-intense and will be executed as late as possible. The resulting **execution plan** consists in a **directed acyclic graph** (DAG) that contains the tree of all required actions for a specific computation, ordered in the most effective fashion.\n",
    "\n",
    "**This has also drawbacks.** Since the computation is optimized for the end result, the intermediate stages are discarded by default. So if you need a DataFrame multiple times, you have to cache it in memory because if you don't Spark will recompute it every single time. \n",
    "\n",
    "---\n",
    "\n",
    "### ‚úçHands-on 2 - Data frame basic manipulations\n",
    "\n",
    "- How many rows have your two DataFrame ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "strong-level",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the small DF has 10000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:18:41,090 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_36_0 !\n",
      "2022-07-13 12:19:04,954 ERROR scheduler.TaskSchedulerImpl: Lost executor 2 on 10.233.86.38: \n",
      "The executor with id 2 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-kubernetes-executor\n",
      "\t container image: docker.io/inseefrlab/jupyter-datascience:latest\n",
      "\t container state: terminated\n",
      "\t container started at: 2022-07-13T11:06:26Z\n",
      "\t container finished at: 2022-07-13T12:18:41Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "2022-07-13 12:19:04,964 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 60.0 (TID 124) (10.233.86.38 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: \n",
      "The executor with id 2 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-kubernetes-executor\n",
      "\t container image: docker.io/inseefrlab/jupyter-datascience:latest\n",
      "\t container state: terminated\n",
      "\t container started at: 2022-07-13T11:06:26Z\n",
      "\t container finished at: 2022-07-13T12:18:41Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "2022-07-13 12:19:11,417 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_204_2 !\n",
      "2022-07-13 12:19:34,976 ERROR scheduler.TaskSchedulerImpl: Lost executor 4 on 10.233.86.95: \n",
      "The executor with id 4 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-kubernetes-executor\n",
      "\t container image: docker.io/inseefrlab/jupyter-datascience:latest\n",
      "\t container state: terminated\n",
      "\t container started at: 2022-07-13T12:17:46Z\n",
      "\t container finished at: 2022-07-13T12:19:11Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "2022-07-13 12:19:34,977 WARN scheduler.TaskSetManager: Lost task 1.1 in stage 60.0 (TID 127) (10.233.86.95 executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: \n",
      "The executor with id 4 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-kubernetes-executor\n",
      "\t container image: docker.io/inseefrlab/jupyter-datascience:latest\n",
      "\t container state: terminated\n",
      "\t container started at: 2022-07-13T12:17:46Z\n",
      "\t container finished at: 2022-07-13T12:19:11Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "2022-07-13 12:19:36,679 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_204_3 !\n",
      "2022-07-13 12:19:36,679 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_195_0 !\n",
      "2022-07-13 12:20:04,999 ERROR scheduler.TaskSchedulerImpl: Lost executor 3 on 10.233.86.11: \n",
      "The executor with id 3 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-kubernetes-executor\n",
      "\t container image: docker.io/inseefrlab/jupyter-datascience:latest\n",
      "\t container state: terminated\n",
      "\t container started at: 2022-07-13T12:17:42Z\n",
      "\t container finished at: 2022-07-13T12:19:36Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "2022-07-13 12:20:05,000 WARN scheduler.TaskSetManager: Lost task 1.2 in stage 60.0 (TID 128) (10.233.86.11 executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: \n",
      "The executor with id 3 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-kubernetes-executor\n",
      "\t container image: docker.io/inseefrlab/jupyter-datascience:latest\n",
      "\t container state: terminated\n",
      "\t container started at: 2022-07-13T12:17:42Z\n",
      "\t container finished at: 2022-07-13T12:19:36Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "2022-07-13 12:20:08,187 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_204_0 !\n",
      "2022-07-13 12:20:08,187 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_204_2 !\n",
      "2022-07-13 12:20:35,024 ERROR scheduler.TaskSchedulerImpl: Lost executor 5 on 10.233.86.151: \n",
      "The executor with id 5 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-kubernetes-executor\n",
      "\t container image: docker.io/inseefrlab/jupyter-datascience:latest\n",
      "\t container state: terminated\n",
      "\t container started at: 2022-07-13T12:17:46Z\n",
      "\t container finished at: 2022-07-13T12:20:08Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "2022-07-13 12:20:35,026 WARN scheduler.TaskSetManager: Lost task 1.3 in stage 60.0 (TID 130) (10.233.86.151 executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: \n",
      "The executor with id 5 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-kubernetes-executor\n",
      "\t container image: docker.io/inseefrlab/jupyter-datascience:latest\n",
      "\t container state: terminated\n",
      "\t container started at: 2022-07-13T12:17:46Z\n",
      "\t container finished at: 2022-07-13T12:20:08Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "2022-07-13 12:20:35,027 ERROR scheduler.TaskSetManager: Task 1 in stage 60.0 failed 4 times; aborting job\n",
      "[Stage 60:=========================================================(5 + -4) / 4]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o258.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 60.0 failed 4 times, most recent failure: Lost task 1.3 in stage 60.0 (TID 130) (10.233.86.151 executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: \nThe executor with id 5 exited with exit code 137(SIGKILL, possible container OOM).\n\n\n\nThe API gave the following container statuses:\n\n\n\t container name: spark-kubernetes-executor\n\t container image: docker.io/inseefrlab/jupyter-datascience:latest\n\t container state: terminated\n\t container started at: 2022-07-13T12:17:46Z\n\t container finished at: 2022-07-13T12:20:08Z\n\t exit code: 137\n\t termination reason: OOMKilled\n      \nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [53]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe small DF has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_tweet\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe big DF has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_tweet_big\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:680\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \n\u001b[1;32m    673\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;124;03m    2\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 680\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o258.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 60.0 failed 4 times, most recent failure: Lost task 1.3 in stage 60.0 (TID 130) (10.233.86.151 executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: \nThe executor with id 5 exited with exit code 137(SIGKILL, possible container OOM).\n\n\n\nThe API gave the following container statuses:\n\n\n\t container name: spark-kubernetes-executor\n\t container image: docker.io/inseefrlab/jupyter-datascience:latest\n\t container state: terminated\n\t container started at: 2022-07-13T12:17:46Z\n\t container finished at: 2022-07-13T12:20:08Z\n\t exit code: 137\n\t termination reason: OOMKilled\n      \nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n"
     ]
    }
   ],
   "source": [
    "print(f'the small DF has {df_tweet.count()} rows')\n",
    "print(f'the big DF has {df_tweet_big.count()} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-weather",
   "metadata": {},
   "source": [
    "- Sample `df_tweet_big` and keep only 10% of it. Create a new DataFrame named `df_tweet_sampled`. If computations take too long on the full DataFrame, use this one instead or add a sample transformation in your expression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-agreement",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_sampled = df_tweet_big.sample(fraction=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-great",
   "metadata": {},
   "source": [
    "- Define a DataFrame `tweet_author_hashtags`  with only the `auteur` and `hashtags` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_author_hashtags = df_tweet_big.select(\"auteur\", \"hashtags\")\n",
    "tweet_author_hashtags.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-anime",
   "metadata": {},
   "source": [
    "- Print (few lines of) a DataFrame with only the `auteur`, `mentions`, and `urls` columns. (`mentions` and `urls` are both nested columns in `entities`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-victoria",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_big.select(\"auteur\", \"entities.urls\", \"entities.mentions\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-calculation",
   "metadata": {},
   "source": [
    "- Filter your first DataFrame and keep only tweets with more than 1 like. Give a name for this new, transformed DataFrame and print. Print (few lines of) it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_big.filter(df_tweet_big.like_count > 0).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-wagon",
   "metadata": {},
   "source": [
    "## ü•àBasic DataFrame column manipulation \n",
    "\n",
    "You can add/update/rename column of a DataFrame with spark :\n",
    "\n",
    "- Drop : `df.drop(columnName : str )`\n",
    "- Rename : `df.withColumnRenamed(oldName : str, newName : str)`\n",
    "- Add/update : `df.withColumn(columnName : str, columnExpression)` \n",
    "\n",
    "For example\n",
    "\n",
    "```python\n",
    "tweet_df_with_like_rt_ratio = tweet_df\\\n",
    "  .withColumn(        # computes new variable\n",
    "    \"like_rt_ratio\", # like_rt_ratio \"OVERCONFIDENCE\"\n",
    "    (tweet_df.like_count /flights.retweet_count\n",
    "   )\n",
    "\n",
    "```\n",
    "\n",
    "See [here](https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.sql.html#functions) for the list of all functions available in an expression.\n",
    "\n",
    "### ‚úçHands-on 3 - Basic DataFrame column manipulation  \n",
    "\n",
    "- Define a DataFrame with a column names `interaction_count`. This column is the sum of `like_count`, `reply_count` and `retweet_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-mixer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_big_interation = df_tweet_big.withColumn(\"interaction_count\", df_tweet_big.like_count+df_tweet_big.reply_count+df_tweet_big.retweet_count )\n",
    "df_tweet_big_interation.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-textbook",
   "metadata": {},
   "source": [
    "- Update the DataFrame you imported at the beginning of this lab and drop the `other` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-state",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_big = df_tweet_big.drop(\"other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-involvement",
   "metadata": {},
   "source": [
    "## ü•áAdvance DataFrame column manipulation \n",
    "\n",
    "### ü•ΩArray manipulation\n",
    "\n",
    "Some columns often contain arrays (lists) of values instead of just one value. This may seem surprising but this actually quite natural. For instance, you may create an array of words from a text, or generate a list of random numbers for each observation, etc.\n",
    "\n",
    "You may **create array of values** with:\n",
    "\n",
    "- `split(text : string, delimiter : string)`, turning a text into an array of strings\n",
    "\n",
    "You may **use array of values** with:\n",
    "\n",
    "- `size(array : Array)`, getting the number of elements\n",
    "\n",
    "- `array_contains(inputArray : Array, value : any)`, checking if some value appears\n",
    "\n",
    "- `explode(array : Array)`, unnesting an array and duplicating other values. For instance it if use `explode()` over the hashtags value of this DataFrame:\n",
    "\n",
    "  | Auteur | Contenu                             | Hashtags         |\n",
    "  | ------ | ----------------------------------- | ---------------- |\n",
    "  | Bob    | I love #Spark and #bigdata          | [Spark, bigdata] |\n",
    "  | Alice  | Just finished #MHrise, best MH ever | [MHrise]         |\n",
    "\n",
    "  I will get :\n",
    "\n",
    "  | Auteur | Contenu                             | Hashtags         | Hashtag |\n",
    "  | ------ | ----------------------------------- | ---------------- | ------- |\n",
    "  | Bob    | I love #Spark and #bigdata          | [Spark, bigdata] | Spark   |\n",
    "  | Bob    | I love #Spark and #bigdata          | [Spark, bigdata] | bigdata |\n",
    "  | Alice  | Just finished #MHrise, best MH ever | [MHrise]         | MHrise  |\n",
    "\n",
    "  \n",
    "\n",
    "All this function must be imported first :\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import split, explode, size, array_contains\n",
    "```\n",
    "\n",
    "Do not forget, to create a new column, you should use `withColumn()`. For example : \n",
    "\n",
    "```python\n",
    "df.withColumn(\"new column\", explode(\"array\"))\n",
    "```\n",
    "\n",
    "#### ‚úçHands-on 4 - Array manipulation \n",
    "\n",
    "- Keep all the tweets with hashtags and for each remaining line, split the hashtag text into an array of hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode, size, array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_big.filter(size(\"hashtags\") > 0).withColumn(\"hashtag\", explode(\"hashtags\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-message",
   "metadata": {},
   "source": [
    "- Create a new column with the number of words of the `contenu` column. (Use `split()` + `size()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-printer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_big.withColumn(\"word_count\", size(split(\"contenu\", \" \"))).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-silly",
   "metadata": {},
   "source": [
    "- Count how many tweet contain the `covid19` hashtag (use the `count()` action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-rapid",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_big.filter(array_contains(\"hashtags\", \"COVID19\")).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-seeker",
   "metadata": {},
   "source": [
    "### ü•ºUser defined function\n",
    "\n",
    "For more very specific column manipulation you will need Spark's `udf()` function (*User Defined Function*). It can be useful if you Spark does not provide a feature you want. But Spark is a popular and active project, so before coding an udf, go check the documentation. For instance for natural language processing, Spark already has some [functions](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.feature.Tokenizer.html#pyspark.ml.feature.Tokenizer). Last things, python udf can lead to performance issues (see https://stackoverflow.com/a/38297050) and learning a little bit of scala or java can be a good idea.\n",
    "\n",
    "For example :\n",
    "\n",
    "```python\n",
    "# !!!! DOES NOT WORK !!!!\n",
    "def to_lower_case(string):\n",
    "\treturn string.lower()\n",
    "\t\n",
    "df.withColumn(\"tweet_lower_case\", to_lower_case(df.contenu))\n",
    "```\n",
    "\n",
    "will just crash. Keep in mind that Spark is a distributed system, and that Python is only installed on the central node, as a convenience to let you execute instructions on the executor nodes. But by default, pure Python functions can only be executed where Python is installed! We need `udf()` to enable Spark to send Python instructions to the worker nodes.\n",
    "\n",
    "Let us see how it is done :\n",
    "\n",
    "```python\n",
    "# imports\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# pure python functions\n",
    "def to_lower_case(string):\n",
    "    return string.lower()\n",
    "\n",
    "# user definid function\n",
    "to_lower_case_udf = udf(\n",
    "    lambda x: to_lower_case(x), StringType()\n",
    ") #we use a lambda function to create the udf.\n",
    "\n",
    "# df manipulation\n",
    "df_tweet_small\\\n",
    "  .select(\"auteur\",\"hashtags\")\\\n",
    "  .filter(\"size(hashtags)!=0\")\\\n",
    "  .withColumn(\"hashtag\", explode(\"hashtags\"))\\\n",
    "  .withColumn(\"hashtag\", to_lower_case_udf(\"hashtag\")).show(10)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úçHands-on 5 - User defined function \n",
    "\n",
    "- Create an user defined function that counts how many words a tweet contains. (your function will return an `IntegerType` and not a `StringType`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# pure python functions\n",
    "def word_count(string):\n",
    "    return len(string.split(\" \"))\n",
    "\n",
    "# user definid function\n",
    "word_count_udf = udf(\n",
    "    lambda x: word_count(x), IntegerType()\n",
    ") #we use a lambda function to create the udf.\n",
    "\n",
    "# df manipulation\n",
    "df_tweet_big\\\n",
    "  .withColumn(\"word_count\", word_count_udf(\"contenu\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-reset",
   "metadata": {},
   "source": [
    "## üî©Aggregation functions\n",
    "\n",
    "Spark offer a variety of aggregation functions :\n",
    "\n",
    "- `count(column : string)` will count every not null value of the specify column. You cant use `count(1)` of `count(\"*\")` to count every line (even row with only null values)\n",
    "\n",
    "- `countDisctinct(column : string)` and `approx_count_distinct(column : string, percent_error: float)`. If the exact number is irrelevant, `approx_count_distinct()`should be preferred.\n",
    "\n",
    "  Counting distinct elements cannot be done in parallel, and need a lot data transfer. But if you only need an approximation, there is a algorithm, named hyper-log-log (more info [here](https://databricks.com/fr/blog/2016/05/19/approximate-algorithms-in-apache-spark-hyperloglog-and-quantiles.html)) that can be parallelized. \n",
    "\n",
    "  ```python\n",
    "  from pyspark.sql.functions import count, countDistinct, approx_count_distinct\n",
    "  \n",
    "  df.select(count(\"col1\")).show()\n",
    "  df.select(countDistinct(\"col1\")).show()\n",
    "  df.select(approx_count_distinct(\"col1\"), 0.1).show()\n",
    "  ```\n",
    "\n",
    "- You have access to all other common functions `min()`, `max()`, `first()`, `last()`, `sum()`, `sumDistinct()`, `avg()` etc (you should import them first `from pyspark.sql.functions import min, max, avg, first, last, sum, sumDistinct`) \n",
    "\n",
    "---\n",
    "\n",
    "### ‚úçHands-on 6 - Aggregation functions\n",
    "\n",
    "- What are the min, max, average of `interaction_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max, avg, first, last, sum, sumDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_big_interation.select(min(\"interaction_count\"),max(\"interaction_count\"),avg(\"interaction_count\")).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-invite",
   "metadata": {},
   "source": [
    "- How many tweets have hashtags ? Distinct hashtags ? Try the approximative count with 0.1 and 0.01as maximum estimation error allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, countDistinct, approx_count_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_big.select(count(\"hashtags\"), countDistinct(\"hashtags\"), approx_count_distinct(\"hashtags\", 0.1), approx_count_distinct(\"hashtags\",0.01)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-enlargement",
   "metadata": {},
   "source": [
    "## Grouping functions\n",
    "\n",
    "Like SQL you can group row by a criteria with Spark. Just use the `groupBy(column : string)` method. Then you can compute some aggregation over those groups.\n",
    "\n",
    "```python\n",
    "df.groupBy(\"col1\").agg(\n",
    "  count(\"col2\").alias(\"quantity\") # alias is use to specify the name of the new column\n",
    ").show() \n",
    "```\n",
    "\n",
    "The `agg()` method can take multiples argument to compute multiple aggregation at once.\n",
    "\n",
    "```python\n",
    "df.groupBy(\"col1\").agg(\n",
    "\tcount(\"col2\").alias(\"quantity\"), min(\"col2\").alias(\"min\"), avg(\"col3\").alias(\"avg3\") ).show()\n",
    "```\n",
    "\n",
    "Aggregation and grouping transformations work differently than the previous method like `filter()`, `select()`, `withColumn()` etc. Those transformations cannot be run over each partitions in parallel, and need to transfer data between partitions and executors.  They are called \"wide transformations\"\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/HealerMikado/panorama_big_data_2021/main/labs/lab%202%20-%20first%20steps%20with%20Spark/img/spark_exemple2_pipeline.png\" style=\"zoom:30%;\" />\n",
    "---\n",
    "\n",
    "### ‚úçHands-on 7 - Grouping functions\n",
    "\n",
    "- Compute a daframe with the min, max and average retweet of each `auteur`. Then order it by the max number of retweet in descending order by . To do that you can use the following syntax\n",
    "\n",
    "  ```python\n",
    "  from pyspark.sql.functions import desc\n",
    "  df.orderBy(desc(\"col\"))\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-astrology",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "df_tweet_big.groupBy(\"auteur\").agg(min(\"retweet_count\").alias(\"min_RT\"), max(\"retweet_count\").alias(\"max_RT\"), avg(\"retweet_count\").alias(\"avg_RT\")).orderBy(desc(\"max_RT\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-surveillance",
   "metadata": {},
   "source": [
    "## üîåSpark SQL\n",
    "\n",
    "Spark understand SQL statement. It's not a hack nor a workaround to use SQL in Spark, it's one a the more powerful feature in Spark. To use SQL in you need :\n",
    "\n",
    "1. Register a view pointing to your DataFrame\n",
    "\n",
    "   ```python\n",
    "   my_df.createOrReplaceTempView(viewName : str)\n",
    "   ```\n",
    "\n",
    "2. Use the sql function\n",
    "\n",
    "   ```python\n",
    "   spark.sql(\"\"\"\n",
    "   You sql statment\n",
    "   \"\"\")\n",
    "   ```\n",
    "\n",
    "   You could manipulate every registered DataFrame by their view name with plain SQL.\n",
    "\n",
    "In fact you can do most of this tutorial without any knowledge in PySpark nor Spark. Lot of things can be done in Sparkk only by only knowing SQL and how to use it in Spark. \n",
    "\n",
    "### ‚úçHands-on 8 - Spark SQL\n",
    "\n",
    "- How many tweets have hashtags ? Distinct hashtags ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-intervention",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_big.select(\"contenu\", \"hashtags\").createOrReplaceTempView(\"view_hashtag_content\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*), COUNT(DISTINCT(contenu))\n",
    "FROM view_hashtag_content\n",
    "WHERE size(hashtags) > 0\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-lindsay",
   "metadata": {},
   "source": [
    "- Compute a dataframe with the min, max and average retweet of each `auteur` using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-variance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_big.createOrReplaceTempView(\"view_tweet_big\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT min(retweet_count), max(retweet_count), avg(retweet_count)\n",
    "FROM view_tweet_big\n",
    "GROUP BY auteur\n",
    "ORDER BY MAX(retweet_count) DESC\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-appliance",
   "metadata": {},
   "source": [
    "## Joins in Spark\n",
    "\n",
    "Like a SQL, Spark can join two dataset  by comparing the value of one or more keys using joins. Joins are by nature wide transformation, so data will be transferred between executors and will take time. But Spark will distinct two cases :\n",
    "\n",
    "- Big table to big table join\n",
    "- Big table to small table join\n",
    "- (The case small table to small table is irrelevant for big data)\n",
    "\n",
    "And optimize the join according to the actual case. (For more information Spark: The Definitive Guide pages 148 - 151)\n",
    "\n",
    "Doing a join is pretty easy. You need :\n",
    "\n",
    "- At least two DataFrames (obviously) with columns with the same keys\n",
    "- a join expression\n",
    "- the join transformation\n",
    "\n",
    "For instance :\n",
    "\n",
    "```python\n",
    "# Creation of 3 small DF\n",
    "person=spark.createDataFrame([\n",
    "    (0,\"Bill Chambers\",0,[100])\n",
    "    ,(1,\"Matei Zaharia\",1,[500,250,100])\n",
    "    ,(2,\"Michael Armbrust\",1,[250,100])])\\\n",
    "\t.toDF(\"id\",\"name\",\"graduate_program\",\"spark_status\")\n",
    "graduateProgram=spark.createDataFrame([\n",
    "    (0,\"Masters\",\"School of Information\",\"UC Berkeley\")\n",
    "    ,(2,\"Masters\",\"EECS\",\"UC Berkeley\"),(1,\"Ph.D.\",\"EECS\",\"UC Berkeley\")])\\\n",
    "\t.toDF(\"id\",\"degree\",\"department\",\"school\")\n",
    "sparkStatus=spark.createDataFrame([\n",
    "    (500,\"Vice President\")\n",
    "    ,(250,\"PMC Member\")\n",
    "    ,(100,\"Contributor\")])\\\n",
    "\t.toDF(\"id\",\"status\")\n",
    "    \n",
    "# A join expression\n",
    "joinExpression=person[\"graduate_program\"]==graduateProgram['id']\n",
    "\n",
    "# The join transformation in action\n",
    "person.join(graduateProgram, joinExpression).show()\n",
    "```\n",
    "\n",
    "By default Spark compute inner joins, but you can pass a third argument to the join transformation with its type. You can do :\n",
    "\n",
    "- Inner joins : by default or with the \"inner\" argument\n",
    "- Outer joins : \"outer\" argument\n",
    "- Left / right outer joins : \"left_outer\", \"right_outer\" argument\n",
    "- Left semi join : it's more a filter than a join. It only keep the row in the left DataFrame that have a match in the right DataFrame. Use he \"left_semi\" argument\n",
    "- Left anti join : The opposite of the previous one. Use he \"left_anti\" argument\n",
    "- You can do cross joins to, but it's a very bad idea to do so, so please don't !\n",
    "\n",
    "### ‚úç Hands-on 9 - Joins in Spark\n",
    "\n",
    "- Import the files stored in `s3n://spark-lab-input-data-ensai20202021/users/` in a DataFrame  and its informations to your DataFrame. Filter your new DataFrame to only keep verified user (`verified == True`) and group by user and get the most active user of your DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, ArrayType, TimestampType, BooleanType\n",
    "schema = StructType([ \\\n",
    "    StructField(\"created_at\",TimestampType(),True), \\\n",
    "    StructField(\"id\",StringType(),True), \\\n",
    "    StructField(\"name\",StringType(),True), \\\n",
    "    StructField(\"username\", StringType(), True), \\\n",
    "    StructField(\"withheld\",StructType([ \n",
    "        StructField(\"country_codes\", ArrayType(StringType(),True)),\\\n",
    "        StructField(\"scope\", StringType(), True)  \\\n",
    "    ])),\n",
    "    StructField(\"verified\",BooleanType(),True)             \n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-official",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_big = spark.read.json(\"s3a://nrandriamanana/diffusion/formation/data/user\")\n",
    "df_user_big.cache()\n",
    "df_user_big.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "planned-collectible",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99e3557fe27426c9ee1c17e5365e88b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      " |-- verified: boolean (nullable = true)\n",
      " |-- withheld: struct (nullable = true)\n",
      " |    |-- country_codes: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- scope: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df_user_big.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "atmospheric-thailand",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_user_big' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_date, lit\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf_user_big\u001b[49m\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated_at\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m>\u001b[39m to_date(lit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2019-01-01\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\u001b[38;5;241m.\u001b[39mcount()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_user_big' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, lit\n",
    "df_user_big.filter(\"created_at\" > to_date(lit(\"2019-01-01\"))).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "corporate-battle",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_user_big' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_user_big\u001b[49m\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcount()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_user_big' is not defined"
     ]
    }
   ],
   "source": [
    "df_user_big.filter(\"verified\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "alien-analyst",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_user_big' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m joinExpression\u001b[38;5;241m=\u001b[39m\u001b[43mdf_user_big\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musername\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39mdf_tweet_big[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauteur\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m df_user_big\u001b[38;5;241m.\u001b[39mjoin(df_tweet_big, joinExpression)\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_user_big' is not defined"
     ]
    }
   ],
   "source": [
    "joinExpression=df_user_big[\"username\"]==df_tweet_big['auteur']\n",
    "\n",
    "df_user_big.join(df_tweet_big, joinExpression).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "acquired-findings",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_user_big' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_user_big\u001b[49m\u001b[38;5;241m.\u001b[39mjoin(df_tweet_big, joinExpression)\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcount()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_user_big' is not defined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 11:27:39,739 WARN k8s.ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)\n",
      "io.fabric8.kubernetes.client.WatcherException: too old resource version: 491337185 (491351069)\n",
      "\tat io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager$TypedWatcherWebSocketListener.onMessage(WatchConnectionManager.java:103)\n",
      "\tat okhttp3.internal.ws.RealWebSocket.onReadMessage(RealWebSocket.java:323)\n",
      "\tat okhttp3.internal.ws.WebSocketReader.readMessageFrame(WebSocketReader.java:219)\n",
      "\tat okhttp3.internal.ws.WebSocketReader.processNextFrame(WebSocketReader.java:105)\n",
      "\tat okhttp3.internal.ws.RealWebSocket.loopReader(RealWebSocket.java:274)\n",
      "\tat okhttp3.internal.ws.RealWebSocket$2.onResponse(RealWebSocket.java:214)\n",
      "\tat okhttp3.RealCall$AsyncCall.execute(RealCall.java:203)\n",
      "\tat okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: io.fabric8.kubernetes.client.KubernetesClientException: too old resource version: 491337185 (491351069)\n",
      "\t... 11 more\n"
     ]
    }
   ],
   "source": [
    "df_user_big.join(df_tweet_big, joinExpression).filter(\"verified\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-fantasy",
   "metadata": {},
   "source": [
    "**DO NOT FORGET TO TURN YOUR CLUSTER OFF!**\n",
    "\n",
    "**DO NOT FORGET TO TURN YOUR CLUSTER OFF!**\n",
    "\n",
    "**DO NOT FORGET TO TURN YOUR CLUSTER OFF!**\n",
    "\n",
    "**DO NOT FORGET TO TURN YOUR CLUSTER OFF!**\n",
    "\n",
    "**DO NOT FORGET TO TURN YOUR CLUSTER OFF!**\n",
    "\n",
    "**DO NOT FORGET TO TURN YOUR CLUSTER OFF!**\n",
    "\n",
    "**DO NOT FORGET TO TURN YOUR CLUSTER OFF!**\n",
    "\n",
    "**DO NOT FORGET TO TURN YOUR CLUSTER OFF!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
